"""Module orchestrating all AI submodules.

æ–‡ä»¶ç»“æ„:

```
UserInput       -> æ•°æ®ç±»
IntelligentCore -> è°ƒåº¦æƒ…ç»ªè¯†åˆ«å’Œå¯¹è¯ç”Ÿæˆ
```

The central brain receives **audio**, **touch** and **image** signals then
passes them through emotion recognition, memory retrieval and personality
growth to produce spoken replies, actions and facial expressions.
ä¸­å¤®å¤§è„‘æ¥æ”¶è¯­éŸ³ã€è§¦æ‘¸å’Œå›¾åƒä¿¡æ¯åï¼Œä¾æ¬¡å®Œæˆæƒ…ç»ªè¯†åˆ«ã€è®°å¿†æŸ¥è¯¢ã€
äººæ ¼æˆé•¿ï¼Œæœ€ç»ˆè¾“å‡ºè¯­éŸ³ã€åŠ¨ä½œä¸è¡¨æƒ…ã€‚
"""

from dataclasses import dataclass
from typing import Optional, Tuple
import asyncio
import logging

from .enhanced_dialogue_engine import EnhancedDialogueEngine, DialogueResponse
from .emotion_perception import EmotionPerception
from .constants import (
    DEFAULT_AUDIO_PATH,
    DEFAULT_IMAGE_PATH,
    LOG_LEVEL,
)

logger = logging.getLogger(__name__)
logger.setLevel(LOG_LEVEL)


@dataclass
class UserInput:
    """Container for incoming interaction data.

    ç”¨æˆ·è¾“å…¥çš„æ•°æ®å®¹å™¨ï¼Œä»… ``robot_id`` ä¸ºå¿…å¡«ï¼Œå…¶ä½™çš†å¯ä¸ºç©ºã€‚åŒ…å«
    æ–‡æœ¬ã€éŸ³é¢‘ã€å›¾åƒã€è§†é¢‘åŠè§¦æ‘¸åŒºåŸŸç¼–å·å…±å…­é¡¹ã€‚"""

    audio_path: str | None = None  # è·¯å¾„å¯ä¸ºç©º
    image_path: str | None = None  # å›¾ç‰‡è·¯å¾„å¯ä¸ºç©º
    video_path: str | None = None  # è§†é¢‘è·¯å¾„å¯ä¸ºç©º
    text: str | None = None        # æ–‡æœ¬å†…å®¹å¯ä¸ºç©º
    robot_id: str = ""             # æœºå™¨äººç¼–å· (å¿…å¡«)
    touch_zone: int | None = None  # è§¦æ‘¸åŒºåŸŸç¼–å·ï¼Œå¯é€‰
    session_id: str | None = None  # ä¼šè¯IDï¼Œç”¨äºä¸Šä¸‹æ–‡è¿ç»­æ€§


class IntelligentCore:
    """Main controller orchestrating submodules.

    æ•´ä½“è°ƒåº¦å„å­æ¨¡å—çš„æ ¸å¿ƒæ§åˆ¶å™¨ã€‚
    """

    def __init__(
        self,
        robot_id: str = "robotA",
        dialogue: Optional[EnhancedDialogueEngine] = None,
        emotion: Optional[EmotionPerception] = None,
        asr_url: str | None = None,
        tts_url: str | None = None,
        llm_url: str | None = None,
        voiceprint_url: str | None = None,
    ) -> None:
        """Initialize dialogue and emotion modules.

        åˆå§‹åŒ–å¯¹è¯ä¸æƒ…ç»ªè¯†åˆ«æ¨¡å—ã€‚

        Parameters
        ----------
        robot_id: str
            æœºå™¨äººID
        dialogue: EnhancedDialogueEngine, optional
            Custom dialogue engine. é»˜è®¤ä¸º :class:`EnhancedDialogueEngine`ã€‚
        emotion: EmotionPerception, optional
            Emotion perception module. é»˜è®¤ä¸º :class:`EmotionPerception`ã€‚
        asr_url: str | None, optional
            Speech recognition service endpoint. ``None`` disables ASR.
        tts_url: str | None, optional
            Text to speech service endpoint.
        llm_url: str | None, optional
            Large language model service endpoint.
        voiceprint_url: str | None, optional
            Speaker identification service endpoint.
        """
        # å¦‚æœæ²¡æœ‰æä¾›llm_urlï¼Œä½¿ç”¨é»˜è®¤å€¼
        if llm_url is None:
            from .constants import DEFAULT_LLM_URL
            llm_url = DEFAULT_LLM_URL
        
        self.robot_id = robot_id
        self.dialogue = dialogue or EnhancedDialogueEngine(
            robot_id=robot_id,
            llm_url=llm_url, 
            tts_url=tts_url
        )  # å¢å¼ºå¯¹è¯ç³»ç»Ÿ
        self.emotion = emotion or EmotionPerception(
            voiceprint_url=voiceprint_url,
            llm_url=llm_url,
            memory=self.dialogue.memory,
            personality=self.dialogue.personality,
        )   # æƒ…ç»ªè¯†åˆ«ç³»ç»Ÿ
        self.asr_url = asr_url
        
        logger.info(f"ğŸ”§ æ™ºèƒ½æ ¸å¿ƒåˆå§‹åŒ–å®Œæˆ")
        logger.info(f"   ğŸ¤– æœºå™¨äººID: {robot_id}")
        logger.info(f"   ğŸ’¬ å¯¹è¯å¼•æ“: {'å·²è¿æ¥' if dialogue else 'æ–°å»º'}")
        logger.info(f"   ğŸ˜Š æƒ…ç»ªè¯†åˆ«: {'å·²è¿æ¥' if emotion else 'æ–°å»º'}")
        logger.info(f"   ğŸ¤ ASRæœåŠ¡: {asr_url or 'æœªé…ç½®'}")
        logger.info(f"   ğŸ”Š TTSæœåŠ¡: {tts_url or 'æœªé…ç½®'}")
        logger.info(f"   ğŸ¤– LLMæœåŠ¡: {llm_url or 'æœªé…ç½®'}")

    def _resolve_paths(self, user: UserInput) -> Tuple[str, str]:
        """Return audio and image paths with fallbacks.

        è¿”å›è§£æç¬¬ä¸€ä½å¯é€‰è·¯å¾„ï¼Œå¦‚ä¸æä¾›åˆ™ä½¿ç”¨é»˜è®¤æ–‡ä»¶ã€‚
        """
        audio_path = user.audio_path or DEFAULT_AUDIO_PATH
        image_or_video = user.image_path or user.video_path or DEFAULT_IMAGE_PATH
        return audio_path, image_or_video

    def _ensure_text(self, user: UserInput, audio_path: str) -> None:
        """Ensure user.text is populated, using ASR if needed.

        ç¡®ä¿ç”¨æˆ·æ–‡æœ¬å·²å¡«å……ï¼Œå¦‚éœ€è¦åˆ™ä½¿ç”¨ASRã€‚
        """
        if not user.text and self.asr_url:
            try:
                from .service_api import call_asr
                user.text = call_asr(audio_path, self.asr_url)
                logger.info(f"ASRè¯†åˆ«ç»“æœ: {user.text}")
            except Exception as e:
                logger.error(f"ASRè°ƒç”¨å¤±è´¥: {e}")
                user.text = ""

    def _perceive(self, audio_path: str, image_or_video: str, text: str) -> Tuple[str, str]:
        """Perceive emotion from multimodal input.

        ä»å¤šæ¨¡æ€è¾“å…¥æ„ŸçŸ¥æƒ…ç»ªã€‚
        """
        try:
            mood_tag, user_id = self.emotion.perceive_emotion(
                audio_path, image_or_video, text
            )
            logger.info(f"æƒ…ç»ªè¯†åˆ«ç»“æœ: {mood_tag}, ç”¨æˆ·ID: {user_id}")
            return mood_tag, user_id
        except Exception as e:
            logger.error(f"æƒ…ç»ªè¯†åˆ«å¤±è´¥: {e}")
            return "neutral", "unknown"

    def process(self, user: UserInput) -> DialogueResponse:
        """Process user input and generate response.

        å¤„ç†ç”¨æˆ·è¾“å…¥å¹¶ç”Ÿæˆå›å¤ã€‚

        Parameters
        ----------
        user: UserInput
            User input data container.

        Returns
        -------
        DialogueResponse
            Generated response with text, audio, actions and expressions.
        """
        logger.info(f"ğŸ¯ å¼€å§‹å¤„ç†ç”¨æˆ·è¾“å…¥ - æœºå™¨äºº: {user.robot_id}")
        logger.info(f"   ğŸ“ æ–‡æœ¬: {user.text}")
        logger.info(f"   ğŸµ éŸ³é¢‘: {user.audio_path}")
        logger.info(f"   ğŸ–¼ï¸ å›¾åƒ: {user.image_path}")
        logger.info(f"   ğŸ¬ è§†é¢‘: {user.video_path}")
        logger.info(f"   ğŸ¤— è§¦æ‘¸åŒºåŸŸ: {user.touch_zone}")
        logger.info(f"   ğŸ†” ä¼šè¯ID: {user.session_id}")

        # 1. è§£æè·¯å¾„
        audio_path, image_or_video = self._resolve_paths(user)
        
        # 2. ç¡®ä¿æ–‡æœ¬å†…å®¹
        self._ensure_text(user, audio_path)
        
        # 3. æƒ…ç»ªæ„ŸçŸ¥
        mood_tag, user_id = self._perceive(audio_path, image_or_video, user.text or "")
        
        # 4. ç”Ÿæˆå›å¤
        touched = user.touch_zone is not None
        
        response = self.dialogue.generate_response(
            user_text=user.text or "",
            mood_tag=mood_tag,
            user_id=user_id,
            touched=touched,
            touch_zone=user.touch_zone,
            session_id=user.session_id,
        )
        
        logger.info(f"âœ… å¤„ç†å®Œæˆ")
        logger.info(f"   ğŸ“ å›å¤æ–‡æœ¬: {response.text}")
        logger.info(f"   ğŸµ éŸ³é¢‘URL: {response.audio}")
        logger.info(f"   ğŸ­ è¡¨æƒ…: {response.expression}")
        logger.info(f"   ğŸ¤¸ åŠ¨ä½œ: {response.action}")
        logger.info(f"   ğŸ†” ä¼šè¯ID: {response.session_id}")
        logger.info(f"   ğŸªŸ ä¸Šä¸‹æ–‡æ‘˜è¦: {response.context_summary}")
        logger.info(f"   ğŸ’¾ è®°å¿†æ•°é‡: {response.memory_count}")
        
        return response

    async def process_async(self, user: UserInput) -> DialogueResponse:
        """Process user input asynchronously.

        å¼‚æ­¥å¤„ç†ç”¨æˆ·è¾“å…¥ã€‚

        Parameters
        ----------
        user: UserInput
            User input data container.

        Returns
        -------
        DialogueResponse
            Generated response with text, audio, actions and expressions.
        """
        # åœ¨å¼‚æ­¥ç¯å¢ƒä¸­è¿è¡ŒåŒæ­¥å¤„ç†
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(None, self.process, user)

    def start_session(self, session_id: Optional[str] = None) -> str:
        """å¼€å§‹æ–°ä¼šè¯"""
        return self.dialogue.start_session(session_id)

    def get_memory_stats(self) -> dict:
        """è·å–è®°å¿†ç»Ÿè®¡ä¿¡æ¯"""
        return self.dialogue.get_memory_stats()

    def clear_session_memory(self, session_id: Optional[str] = None) -> int:
        """æ¸…é™¤ä¼šè¯è®°å¿†"""
        return self.dialogue.clear_session_memory(session_id)

    def close(self):
        """å…³é—­æ™ºèƒ½æ ¸å¿ƒ"""
        if hasattr(self, 'dialogue'):
            self.dialogue.close()
        logger.info("ğŸ”’ æ™ºèƒ½æ ¸å¿ƒå·²å…³é—­")