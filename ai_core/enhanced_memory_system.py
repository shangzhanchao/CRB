"""Enhanced Memory System for Companion Robot

å¢å¼ºè®°å¿†ç³»ç»Ÿï¼Œæä¾›å¤šå±‚æ¬¡ã€æ™ºèƒ½åŒ–çš„è®°å¿†ç®¡ç†ã€‚
åŒ…å«çŸ­æœŸè®°å¿†ã€é•¿æœŸè®°å¿†ã€ä¸Šä¸‹æ–‡è®°å¿†å’Œæƒ…æ„Ÿè®°å¿†ã€‚
"""

import datetime
import json
import logging
import sqlite3
import threading
from dataclasses import dataclass, asdict
from typing import List, Dict, Any, Optional, Tuple, Deque
from collections import deque
import hashlib
import uuid

try:
    import numpy as np
    from sentence_transformers import SentenceTransformer
except ImportError:
    np = None
    SentenceTransformer = None

from .constants import LOG_LEVEL
from ai_core.constants import HISTORY_MAX_RECORDS

logger = logging.getLogger(__name__)
logger.setLevel(LOG_LEVEL)


@dataclass
class MemoryRecord:
    """è®°å¿†è®°å½•æ•°æ®ç»“æ„"""
    id: str                           # å”¯ä¸€æ ‡è¯†
    robot_id: str                     # æœºå™¨äººID
    timestamp: datetime.datetime       # æ—¶é—´æˆ³
    user_text: str                    # ç”¨æˆ·è¾“å…¥
    ai_response: str                  # AIå›å¤
    mood_tag: str                     # æƒ…ç»ªæ ‡ç­¾
    touch_zone: Optional[int]         # è§¦æ‘¸åŒºåŸŸ
    context_id: str                   # ä¸Šä¸‹æ–‡ID
    session_id: str                   # ä¼šè¯ID
    importance_score: float           # é‡è¦æ€§è¯„åˆ†
    memory_type: str                  # è®°å¿†ç±»å‹
    vector: Optional[List[float]]     # è¯­ä¹‰å‘é‡
    metadata: Dict[str, Any]         # å…ƒæ•°æ®


@dataclass
class ContextWindow:
    """ä¸Šä¸‹æ–‡çª—å£æ•°æ®ç»“æ„"""
    session_id: str                   # ä¼šè¯ID
    robot_id: str                     # æœºå™¨äººID
    records: Deque[MemoryRecord]      # è®°å¿†è®°å½•é˜Ÿåˆ—
    max_size: int                     # æœ€å¤§è®°å½•æ•°
    current_context: str              # å½“å‰ä¸Šä¸‹æ–‡æ‘˜è¦
    emotion_trend: List[str]          # æƒ…ç»ªè¶‹åŠ¿
    interaction_count: int            # äº¤äº’æ¬¡æ•°


class EnhancedMemorySystem:
    """å¢å¼ºè®°å¿†ç³»ç»Ÿ
    
    æä¾›å¤šå±‚æ¬¡è®°å¿†ç®¡ç†ï¼š
    1. çŸ­æœŸè®°å¿†ï¼ˆä¼šè¯è®°å¿†ï¼‰
    2. é•¿æœŸè®°å¿†ï¼ˆè¯­ä¹‰è®°å¿†ï¼‰
    3. ä¸Šä¸‹æ–‡è®°å¿†ï¼ˆå¯¹è¯è¿ç»­æ€§ï¼‰
    4. æƒ…æ„Ÿè®°å¿†ï¼ˆæƒ…ç»ªå…³è”ï¼‰
    """

    def __init__(
        self,
        robot_id: str,
        db_path: str = "enhanced_memory.db",
        session_limit: int = 20,
        context_window_size: int = 10,
        vector_dim: int = 384,
        model_name: str = "all-MiniLM-L6-v2",
    ):
        """åˆå§‹åŒ–å¢å¼ºè®°å¿†ç³»ç»Ÿ
        
        Parameters
        ----------
        robot_id: str
            æœºå™¨äººID
        db_path: str
            æ•°æ®åº“è·¯å¾„
        session_limit: int
            ä¼šè¯è®°å¿†é™åˆ¶
        context_window_size: int
            ä¸Šä¸‹æ–‡çª—å£å¤§å°
        vector_dim: int
            å‘é‡ç»´åº¦
        model_name: str
            å‘é‡æ¨¡å‹åç§°
        """
        self.robot_id = robot_id
        self.db_path = db_path
        self.session_limit = session_limit
        self.context_window_size = context_window_size
        self.vector_dim = vector_dim
        
        # åˆå§‹åŒ–å‘é‡æ¨¡å‹
        self.transformer = None
        if SentenceTransformer is not None:
            try:
                self.transformer = SentenceTransformer(model_name)
                test_vec = self.transformer.encode(["test"])[0]
                self.vector_dim = len(test_vec)
                logger.info(f"âœ… å‘é‡æ¨¡å‹åŠ è½½æˆåŠŸ: {model_name}")
            except Exception as e:
                logger.warning(f"âš ï¸ å‘é‡æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        
        # åˆå§‹åŒ–æ•°æ®åº“
        self._init_database()
        
        # ä¼šè¯è®°å¿†ï¼ˆçŸ­æœŸè®°å¿†ï¼‰
        self.session_memory = deque(maxlen=session_limit)
        
        # ä¸Šä¸‹æ–‡çª—å£
        self.context_windows: Dict[str, ContextWindow] = {}
        
        # å½“å‰ä¼šè¯ID
        self.current_session_id = None
        
        # çº¿ç¨‹é”ï¼Œç”¨äºä¿æŠ¤æ•°æ®åº“æ“ä½œ
        self._db_lock = threading.Lock()
        
        logger.info(f"ğŸ”§ å¢å¼ºè®°å¿†ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        logger.info(f"   ğŸ¤– æœºå™¨äººID: {robot_id}")
        logger.info(f"   ğŸ“Š ä¼šè¯é™åˆ¶: {session_limit}")
        logger.info(f"   ğŸªŸ ä¸Šä¸‹æ–‡çª—å£: {context_window_size}")
        logger.info(f"   ğŸ§  å‘é‡ç»´åº¦: {self.vector_dim}")
    
    def _init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“è¡¨ç»“æ„"""
        self.conn = sqlite3.connect(self.db_path, check_same_thread=False)
        self.conn.row_factory = sqlite3.Row
        
        cursor = self.conn.cursor()
        
        # åˆ›å»ºè®°å¿†è®°å½•è¡¨
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS memory_records (
                id TEXT PRIMARY KEY,
                robot_id TEXT NOT NULL,
                timestamp TEXT NOT NULL,
                user_text TEXT NOT NULL,
                ai_response TEXT NOT NULL,
                mood_tag TEXT NOT NULL,
                touch_zone INTEGER,
                context_id TEXT NOT NULL,
                session_id TEXT NOT NULL,
                importance_score REAL NOT NULL,
                memory_type TEXT NOT NULL,
                vector TEXT,
                metadata TEXT,
                created_at TEXT DEFAULT CURRENT_TIMESTAMP
            )
        """)
        
        # åˆ›å»ºä¼šè¯è¡¨
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS sessions (
                session_id TEXT PRIMARY KEY,
                robot_id TEXT NOT NULL,
                start_time TEXT NOT NULL,
                end_time TEXT,
                interaction_count INTEGER DEFAULT 0,
                emotion_summary TEXT,
                context_summary TEXT,
                created_at TEXT DEFAULT CURRENT_TIMESTAMP
            )
        """)
        
        # åˆ›å»ºä¸Šä¸‹æ–‡è¡¨
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS contexts (
                context_id TEXT PRIMARY KEY,
                session_id TEXT NOT NULL,
                robot_id TEXT NOT NULL,
                context_summary TEXT,
                emotion_trend TEXT,
                record_count INTEGER DEFAULT 0,
                created_at TEXT DEFAULT CURRENT_TIMESTAMP,
                updated_at TEXT DEFAULT CURRENT_TIMESTAMP
            )
        """)
        
        # åˆ›å»ºç´¢å¼•
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_robot_id ON memory_records(robot_id)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_session_id ON memory_records(session_id)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_context_id ON memory_records(context_id)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_timestamp ON memory_records(timestamp)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_memory_type ON memory_records(memory_type)")
        
        self.conn.commit()
        logger.info("âœ… æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")
    
    def _embed_text(self, text: str) -> List[float]:
        """æ–‡æœ¬å‘é‡åŒ–"""
        if self.transformer is None:
            # å¦‚æœæ²¡æœ‰å‘é‡æ¨¡å‹ï¼Œè¿”å›éšæœºå‘é‡
            import random
            return [random.random() for _ in range(self.vector_dim)]
        
        try:
            vector = self.transformer.encode([text])[0]
            return vector.tolist()
        except Exception as e:
            logger.error(f"âŒ å‘é‡åŒ–å¤±è´¥: {e}")
            return [0.0] * self.vector_dim
    
    def _calculate_importance_score(
        self,
        user_text: str,
        ai_response: str,
        mood_tag: str,
        touch_zone: Optional[int],
        interaction_count: int,
    ) -> float:
        """è®¡ç®—è®°å¿†é‡è¦æ€§è¯„åˆ†"""
        score = 0.0
        
        # åŸºç¡€åˆ†æ•°
        score += 0.3
        
        # æ–‡æœ¬é•¿åº¦å½±å“
        text_length = len(user_text) + len(ai_response)
        score += min(text_length / 1000, 0.2)
        
        # æƒ…ç»ªå½±å“
        emotion_scores = {
            "happy": 0.3,
            "excited": 0.4,
            "sad": 0.2,
            "angry": 0.3,
            "surprised": 0.3,
            "neutral": 0.1
        }
        score += emotion_scores.get(mood_tag, 0.1)
        
        # è§¦æ‘¸å½±å“
        if touch_zone is not None:
            score += 0.2
        
        # äº¤äº’é¢‘ç‡å½±å“ï¼ˆæ—©æœŸäº¤äº’æ›´é‡è¦ï¼‰
        if interaction_count <= 5:
            score += 0.3
        elif interaction_count <= 10:
            score += 0.1
        
        # å…³é”®è¯æ£€æµ‹
        important_keywords = [
            "å–œæ¬¢", "è®¨åŒ", "é‡è¦", "è®°ä½", "å¿˜è®°", "åå­—", "ç”Ÿæ—¥",
            "å®¶", "æœ‹å‹", "å·¥ä½œ", "å­¦ä¹ ", "æ¢¦æƒ³", "ç›®æ ‡"
        ]
        for keyword in important_keywords:
            if keyword in user_text or keyword in ai_response:
                score += 0.2
                break
        
        return min(score, 1.0)
    
    def start_session(self, session_id: Optional[str] = None) -> str:
        """å¼€å§‹æ–°ä¼šè¯"""
        if session_id is None:
            session_id = str(uuid.uuid4())
        
        self.current_session_id = session_id
        
        # åˆ›å»ºä¸Šä¸‹æ–‡çª—å£
        context_window = ContextWindow(
            session_id=session_id,
            robot_id=self.robot_id,
            records=deque(maxlen=self.context_window_size),
            max_size=self.context_window_size,
            current_context="",
            emotion_trend=[],
            interaction_count=0
        )
        
        self.context_windows[session_id] = context_window
        
        # è®°å½•åˆ°æ•°æ®åº“
        with self._db_lock:
            cursor = self.conn.cursor()
            cursor.execute("""
                INSERT OR REPLACE INTO sessions 
                (session_id, robot_id, start_time, interaction_count)
                VALUES (?, ?, ?, ?)
            """, (session_id, self.robot_id, datetime.datetime.utcnow().isoformat(), 0))
            self.conn.commit()
        
        logger.info(f"ğŸ†• å¼€å§‹æ–°ä¼šè¯: {session_id}")
        return session_id
    
    def add_memory(
        self,
        user_text: str,
        ai_response: str,
        mood_tag: str = "neutral",
        touch_zone: Optional[int] = None,
        session_id: Optional[str] = None,
        context_id: Optional[str] = None,
    ) -> str:
        """æ·»åŠ è®°å¿†è®°å½•"""
        if session_id is None:
            session_id = self.current_session_id or self.start_session()
        
        if context_id is None:
            context_id = str(uuid.uuid4())
        
        # è·å–ä¸Šä¸‹æ–‡çª—å£
        context_window = self.context_windows.get(session_id)
        if context_window is None:
            context_window = ContextWindow(
                session_id=session_id,
                robot_id=self.robot_id,
                records=deque(maxlen=self.context_window_size),
                max_size=self.context_window_size,
                current_context="",
                emotion_trend=[],
                interaction_count=0
            )
            self.context_windows[session_id] = context_window
        
        # æ›´æ–°äº¤äº’è®¡æ•°
        context_window.interaction_count += 1
        
        # è®¡ç®—é‡è¦æ€§è¯„åˆ†
        importance_score = self._calculate_importance_score(
            user_text, ai_response, mood_tag, touch_zone, context_window.interaction_count
        )
        
        # ç”Ÿæˆå‘é‡
        combined_text = f"{user_text} {ai_response}"
        vector = self._embed_text(combined_text)
        
        # åˆ›å»ºè®°å¿†è®°å½•
        memory_id = str(uuid.uuid4())
        record = MemoryRecord(
            id=memory_id,
            robot_id=self.robot_id,
            timestamp=datetime.datetime.utcnow(),
            user_text=user_text,
            ai_response=ai_response,
            mood_tag=mood_tag,
            touch_zone=touch_zone,
            context_id=context_id,
            session_id=session_id,
            importance_score=importance_score,
            memory_type="interaction",
            vector=vector,
            metadata={
                "interaction_count": context_window.interaction_count,
                "session_length": len(context_window.records) + 1
            }
        )
        
        # æ·»åŠ åˆ°ä¼šè¯è®°å¿†
        context_window.records.append(record)
        context_window.emotion_trend.append(mood_tag)
        
        # æ›´æ–°ä¸Šä¸‹æ–‡æ‘˜è¦
        self._update_context_summary(context_window)
        
        # å­˜å‚¨åˆ°æ•°æ®åº“
        self._save_memory_record(record)
        
        # æ›´æ–°ä¼šè¯ç»Ÿè®¡
        self._update_session_stats(session_id, context_window)
        
        logger.info(f"ğŸ’¾ æ·»åŠ è®°å¿†è®°å½•: {memory_id}")
        logger.info(f"   ğŸ“ ç”¨æˆ·: {user_text[:50]}...")
        logger.info(f"   ğŸ¤– AI: {ai_response[:50]}...")
        logger.info(f"   ğŸ˜Š æƒ…ç»ª: {mood_tag}")
        logger.info(f"   â­ é‡è¦æ€§: {importance_score:.2f}")
        
        return memory_id
    
    def _save_memory_record(self, record: MemoryRecord):
        """ä¿å­˜è®°å¿†è®°å½•åˆ°æ•°æ®åº“"""
        with self._db_lock:
            cursor = self.conn.cursor()
            cursor.execute("""
                INSERT INTO memory_records 
                (id, robot_id, timestamp, user_text, ai_response, mood_tag, 
                 touch_zone, context_id, session_id, importance_score, 
                 memory_type, vector, metadata)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                record.id,
                record.robot_id,
                record.timestamp.isoformat(),
                record.user_text,
                record.ai_response,
                record.mood_tag,
                record.touch_zone,
                record.context_id,
                record.session_id,
                record.importance_score,
                record.memory_type,
                json.dumps(record.vector) if record.vector else None,
                json.dumps(record.metadata)
            ))
            self.conn.commit()
    
    def _update_context_summary(self, context_window: ContextWindow):
        """æ›´æ–°ä¸Šä¸‹æ–‡æ‘˜è¦"""
        if not context_window.records:
            return
        
        # è·å–æœ€è¿‘çš„è®°å½•
        recent_records = list(context_window.records)[-3:]
        
        # æ„å»ºä¸Šä¸‹æ–‡æ‘˜è¦
        summary_parts = []
        for record in recent_records:
            summary_parts.append(f"ç”¨æˆ·è¯´'{record.user_text}'ï¼Œæˆ‘å›å¤'{record.ai_response}'")
        
        context_window.current_context = "ã€‚".join(summary_parts) + "ã€‚"
        
        # æ›´æ–°æƒ…ç»ªè¶‹åŠ¿
        if len(context_window.emotion_trend) > 5:
            context_window.emotion_trend = context_window.emotion_trend[-5:]
    
    def _update_session_stats(self, session_id: str, context_window: ContextWindow):
        """æ›´æ–°ä¼šè¯ç»Ÿè®¡ä¿¡æ¯"""
        with self._db_lock:
            cursor = self.conn.cursor()
            
            # è®¡ç®—æƒ…ç»ªæ‘˜è¦
            emotion_counts = {}
            for emotion in context_window.emotion_trend:
                emotion_counts[emotion] = emotion_counts.get(emotion, 0) + 1
            
            dominant_emotion = max(emotion_counts.items(), key=lambda x: x[1])[0] if emotion_counts else "neutral"
            
            cursor.execute("""
                UPDATE sessions 
                SET interaction_count = ?, emotion_summary = ?, context_summary = ?
                WHERE session_id = ?
            """, (
                context_window.interaction_count,
                dominant_emotion,
                context_window.current_context,
                session_id
            ))
            self.conn.commit()
    
    def query_memory(
        self,
        prompt: str,
        top_k: int = 5,
        session_id: Optional[str] = None,
        memory_types: Optional[List[str]] = None,
        use_context: bool = True,
    ) -> Dict[str, Any]:
        """æŸ¥è¯¢ç›¸å…³è®°å¿†"""
        if memory_types is None:
            memory_types = ["interaction", "semantic", "emotional"]
        
        # 1. æŸ¥è¯¢è¯­ä¹‰ç›¸ä¼¼è®°å¿†
        semantic_memories = self._query_semantic_memory(prompt, top_k)
        
        # 2. æŸ¥è¯¢ä¸Šä¸‹æ–‡è®°å¿†
        context_memories = []
        if use_context and session_id:
            context_memories = self._query_context_memory(session_id, top_k)
        
        # 3. æŸ¥è¯¢æƒ…æ„Ÿç›¸å…³è®°å¿†
        emotional_memories = self._query_emotional_memory(prompt, top_k)
        
        # 4. èåˆè®°å¿†ç»“æœ
        fused_memories = self._fuse_memories(
            semantic_memories, context_memories, emotional_memories, top_k
        )
        
        # 5. ç”Ÿæˆè®°å¿†æ‘˜è¦
        memory_summary = self._generate_memory_summary(fused_memories)
        
        result = {
            "memories": fused_memories,
            "summary": memory_summary,
            "count": len(fused_memories),
            "types": {
                "semantic": len(semantic_memories),
                "context": len(context_memories),
                "emotional": len(emotional_memories)
            }
        }
        
        logger.info(f"ğŸ” è®°å¿†æŸ¥è¯¢ç»“æœ:")
        logger.info(f"   ğŸ“Š æ€»è®°å¿†æ•°: {len(fused_memories)}")
        logger.info(f"   ğŸ§  è¯­ä¹‰è®°å¿†: {len(semantic_memories)}")
        logger.info(f"   ğŸªŸ ä¸Šä¸‹æ–‡è®°å¿†: {len(context_memories)}")
        logger.info(f"   ğŸ˜Š æƒ…æ„Ÿè®°å¿†: {len(emotional_memories)}")
        logger.info(f"   ğŸ“ è®°å¿†æ‘˜è¦: {memory_summary[:100]}...")
        
        return result
    
    def _query_semantic_memory(self, prompt: str, top_k: int) -> List[Dict[str, Any]]:
        """æŸ¥è¯¢è¯­ä¹‰ç›¸ä¼¼è®°å¿†"""
        if self.transformer is None:
            return []
        
        try:
            query_vector = self._embed_text(prompt)
            
            with self._db_lock:
                cursor = self.conn.cursor()
                cursor.execute("""
                    SELECT * FROM memory_records 
                    WHERE robot_id = ? AND vector IS NOT NULL
                    ORDER BY timestamp DESC
                    LIMIT 100
                """, (self.robot_id,))
                
                records = cursor.fetchall()
                if not records:
                    return []
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            similarities = []
            for record in records:
                try:
                    vector = json.loads(record["vector"])
                    similarity = self._cosine_similarity(query_vector, vector)
                    similarities.append((similarity, dict(record)))
                except Exception as e:
                    logger.warning(f"âš ï¸ å‘é‡è§£æå¤±è´¥: {e}")
                    continue
            
            # æŒ‰ç›¸ä¼¼åº¦æ’åº
            similarities.sort(key=lambda x: x[0], reverse=True)
            
            return [record for _, record in similarities[:top_k]]
            
        except Exception as e:
            logger.error(f"âŒ è¯­ä¹‰è®°å¿†æŸ¥è¯¢å¤±è´¥: {e}")
            return []
    
    def _query_context_memory(self, session_id: str, top_k: int) -> List[Dict[str, Any]]:
        """æŸ¥è¯¢ä¸Šä¸‹æ–‡è®°å¿†"""
        with self._db_lock:
            cursor = self.conn.cursor()
            cursor.execute("""
                SELECT * FROM memory_records 
                WHERE session_id = ? 
                ORDER BY timestamp DESC 
                LIMIT ?
            """, (session_id, top_k))
            
            return [dict(record) for record in cursor.fetchall()]
    
    def _query_emotional_memory(self, prompt: str, top_k: int) -> List[Dict[str, Any]]:
        """æŸ¥è¯¢æƒ…æ„Ÿç›¸å…³è®°å¿†"""
        # ç®€å•çš„æƒ…æ„Ÿå…³é”®è¯åŒ¹é…
        emotion_keywords = {
            "happy": ["å¼€å¿ƒ", "é«˜å…´", "å¿«ä¹", "å…´å¥‹", "æ„‰å¿«"],
            "sad": ["éš¾è¿‡", "ä¼¤å¿ƒ", "æ‚²ä¼¤", "æ²®ä¸§", "å¤±æœ›"],
            "angry": ["ç”Ÿæ°”", "æ„¤æ€’", "æ¼ç«", "çƒ¦èº"],
            "surprised": ["æƒŠè®¶", "éœ‡æƒŠ", "æ„å¤–", "åƒæƒŠ"],
            "excited": ["æ¿€åŠ¨", "å…´å¥‹", "çƒ­æƒ…", "æŒ¯å¥‹"]
        }
        
        detected_emotions = []
        for emotion, keywords in emotion_keywords.items():
            for keyword in keywords:
                if keyword in prompt:
                    detected_emotions.append(emotion)
                    break
        
        if not detected_emotions:
            return []
        
        # æŸ¥è¯¢ç›¸å…³æƒ…æ„Ÿè®°å¿†
        with self._db_lock:
            cursor = self.conn.cursor()
            placeholders = ",".join(["?"] * len(detected_emotions))
            cursor.execute(f"""
                SELECT * FROM memory_records 
                WHERE robot_id = ? AND mood_tag IN ({placeholders})
                ORDER BY importance_score DESC, timestamp DESC 
                LIMIT ?
            """, (self.robot_id, *detected_emotions, top_k))
            
            return [dict(record) for record in cursor.fetchall()]
    
    def _cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        if len(vec1) != len(vec2):
            return 0.0
        
        dot_product = sum(a * b for a, b in zip(vec1, vec2))
        norm1 = sum(a * a for a in vec1) ** 0.5
        norm2 = sum(b * b for b in vec2) ** 0.5
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        return dot_product / (norm1 * norm2)
    
    def _fuse_memories(
        self,
        semantic_memories: List[Dict[str, Any]],
        context_memories: List[Dict[str, Any]],
        emotional_memories: List[Dict[str, Any]],
        top_k: int
    ) -> List[Dict[str, Any]]:
        """èåˆä¸åŒç±»å‹çš„è®°å¿†"""
        all_memories = []
        
        # æ·»åŠ è¯­ä¹‰è®°å¿†ï¼ˆæƒé‡1.0ï¼‰
        for memory in semantic_memories:
            memory["weight"] = 1.0
            memory["source"] = "semantic"
            all_memories.append(memory)
        
        # æ·»åŠ ä¸Šä¸‹æ–‡è®°å¿†ï¼ˆæƒé‡1.2ï¼‰
        for memory in context_memories:
            memory["weight"] = 1.2
            memory["source"] = "context"
            all_memories.append(memory)
        
        # æ·»åŠ æƒ…æ„Ÿè®°å¿†ï¼ˆæƒé‡1.1ï¼‰
        for memory in emotional_memories:
            memory["weight"] = 1.1
            memory["source"] = "emotional"
            all_memories.append(memory)
        
        # å»é‡å¹¶æ’åº
        seen_ids = set()
        unique_memories = []
        
        for memory in all_memories:
            memory_id = memory["id"]
            if memory_id not in seen_ids:
                seen_ids.add(memory_id)
                unique_memories.append(memory)
        
        # æŒ‰æƒé‡å’Œé‡è¦æ€§æ’åº
        unique_memories.sort(
            key=lambda x: x["weight"] * x.get("importance_score", 0.5),
            reverse=True
        )
        
        return unique_memories[:top_k]
    
    def _generate_memory_summary(self, memories: List[Dict[str, Any]]) -> str:
        """ç”Ÿæˆè®°å¿†æ‘˜è¦"""
        if not memories:
            return ""
        
        # é€‰æ‹©æœ€é‡è¦çš„è®°å¿†
        important_memories = sorted(
            memories, 
            key=lambda x: x.get("importance_score", 0.5), 
            reverse=True
        )[:3]
        
        summary_parts = []
        for memory in important_memories:
            user_text = memory["user_text"]
            ai_response = memory["ai_response"]
            mood_tag = memory["mood_tag"]
            
            summary_parts.append(
                f"ç”¨æˆ·è¯´'{user_text}'æ—¶ï¼Œæˆ‘å›å¤'{ai_response}'ï¼Œå½“æ—¶æƒ…ç»ªæ˜¯{mood_tag}"
            )
        
        return "ã€‚".join(summary_parts) + "ã€‚"
    
    def get_current_context(self, session_id: Optional[str] = None) -> str:
        """è·å–å½“å‰ä¸Šä¸‹æ–‡"""
        if session_id is None:
            session_id = self.current_session_id
        
        if session_id is None:
            return ""
        
        context_window = self.context_windows.get(session_id)
        if context_window is None:
            return ""
        
        return context_window.current_context
    
    def get_recent_memories(self, session_id: str, limit: int = HISTORY_MAX_RECORDS) -> List[Dict[str, Any]]:
        """
        è·å–æœ€è¿‘çš„è®°å¿†è®°å½•ï¼Œç”¨äºæ„å»ºå†å²å¯¹è¯
        
        Parameters
        ----------
        session_id : str
            ä¼šè¯ID
        limit : int
            è·å–çš„è®°å½•æ•°é‡é™åˆ¶
            
        Returns
        -------
        List[Dict[str, Any]]
            æœ€è¿‘çš„è®°å¿†è®°å½•åˆ—è¡¨
        """
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                
                # æŸ¥è¯¢æœ€è¿‘çš„è®°å¿†è®°å½•
                query = """
                SELECT user_text, ai_response, mood_tag, touch_zone, timestamp
                FROM memory_records 
                WHERE session_id = ? AND robot_id = ?
                ORDER BY timestamp DESC
                LIMIT ?
                """
                
                cursor.execute(query, (session_id, self.robot_id, limit))
                rows = cursor.fetchall()
                
                # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
                memories = []
                for row in rows:
                    memory = {
                        'user_text': row[0],
                        'ai_response': row[1],
                        'mood_tag': row[2],
                        'touch_zone': row[3],
                        'timestamp': row[4]
                    }
                    memories.append(memory)
                
                # æŒ‰æ—¶é—´é¡ºåºæ’åˆ—ï¼ˆæœ€æ—©çš„åœ¨å‰ï¼‰
                memories.reverse()
                
                logger.info(f"ğŸ“ è·å–æœ€è¿‘è®°å¿†: {len(memories)}æ¡è®°å½• (ä¼šè¯: {session_id})")
                return memories
                
        except Exception as e:
            logger.error(f"âŒ è·å–æœ€è¿‘è®°å¿†å¤±è´¥: {e}")
            return []

    def get_memory_stats(self) -> Dict[str, Any]:
        """è·å–è®°å¿†ç»Ÿè®¡ä¿¡æ¯"""
        with self._db_lock:
            cursor = self.conn.cursor()
            
            # æ€»è®°å½•æ•°
            cursor.execute("SELECT COUNT(*) FROM memory_records WHERE robot_id = ?", (self.robot_id,))
            total_records = cursor.fetchone()[0]
            
            # ä¼šè¯æ•°
            cursor.execute("SELECT COUNT(*) FROM sessions WHERE robot_id = ?", (self.robot_id,))
            total_sessions = cursor.fetchone()[0]
            
            # æƒ…ç»ªåˆ†å¸ƒ
            cursor.execute("""
                SELECT mood_tag, COUNT(*) as count 
                FROM memory_records 
                WHERE robot_id = ? 
                GROUP BY mood_tag
            """, (self.robot_id,))
            emotion_distribution = dict(cursor.fetchall())
            
            # é‡è¦æ€§åˆ†å¸ƒ
            cursor.execute("""
                SELECT 
                    CASE 
                        WHEN importance_score >= 0.8 THEN 'high'
                        WHEN importance_score >= 0.5 THEN 'medium'
                        ELSE 'low'
                    END as importance_level,
                    COUNT(*) as count
                FROM memory_records 
                WHERE robot_id = ?
                GROUP BY importance_level
            """, (self.robot_id,))
            importance_distribution = dict(cursor.fetchall())
            
            return {
                "total_records": total_records,
                "total_sessions": total_sessions,
                "emotion_distribution": emotion_distribution,
                "importance_distribution": importance_distribution,
                "active_sessions": len(self.context_windows),
                "vector_dim": self.vector_dim
            }
    
    def clear_session_memory(self, session_id: Optional[str] = None) -> int:
        """æ¸…é™¤ä¼šè¯è®°å¿†"""
        if session_id is None:
            session_id = self.current_session_id
        
        if session_id is None:
            return 0
        
        # ä»å†…å­˜ä¸­ç§»é™¤
        if session_id in self.context_windows:
            removed_count = len(self.context_windows[session_id].records)
            del self.context_windows[session_id]
        else:
            removed_count = 0
        
        # ä»æ•°æ®åº“ä¸­åˆ é™¤
        with self._db_lock:
            cursor = self.conn.cursor()
            cursor.execute("DELETE FROM memory_records WHERE session_id = ?", (session_id,))
            cursor.execute("DELETE FROM sessions WHERE session_id = ?", (session_id,))
            self.conn.commit()
        
        logger.info(f"ğŸ—‘ï¸ æ¸…é™¤ä¼šè¯è®°å¿†: {session_id}, åˆ é™¤è®°å½•æ•°: {removed_count}")
        return removed_count
    
    def close(self):
        """å…³é—­è®°å¿†ç³»ç»Ÿ"""
        with self._db_lock:
            if hasattr(self, 'conn'):
                self.conn.close()
        logger.info("ğŸ”’ å¢å¼ºè®°å¿†ç³»ç»Ÿå·²å…³é—­") 