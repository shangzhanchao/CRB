"""Command line demo for the Companion Robot Intelligent Brain.

å‘½ä»¤è¡Œæ¼”ç¤ºè„šæœ¬ï¼Œå±•ç¤ºå¦‚ä½•å‘æ™ºèƒ½å¤§è„‘å‘é€ä¸€æ¬¡äº¤äº’è¯·æ±‚ã€‚

Usage examples::

    python demo.py --robot_id robotA --text "ä½ å¥½"
    python demo.py --robot_id robotA --audio voice.wav --image face.png --touch_zone 1
"""

import argparse
import json
import logging
import sys

from ai_core import IntelligentCore, UserInput
from ai_core.constants import LOG_LEVEL

# é…ç½®è¯¦ç»†çš„æ—¥å¿—è¾“å‡º
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('robot_demo.log', encoding='utf-8')
    ]
)


def main() -> None:
    """Run a single interaction using command line parameters.

    é€šè¿‡å‘½ä»¤è¡Œå‚æ•°æ‰§è¡Œä¸€æ¬¡äº¤äº’ã€‚
    """
    parser = argparse.ArgumentParser(description="Demo for the robot brain")
    parser.add_argument("--robot_id", required=True, help="robot identifier")
    parser.add_argument("--text", help="user text input")
    parser.add_argument("--audio", help="path to audio file")
    parser.add_argument("--image", help="path to face image")
    parser.add_argument("--video", help="path to video clip")
    parser.add_argument(
        "--touch_zone",
        type=int,
        choices=[0, 1, 2],
        help="touch sensor zone: 0=head, 1=back, 2=chest",
    )
    parser.add_argument("--llm", help="LLM service URL")
    parser.add_argument("--show_status", action="store_true", help="æ˜¾ç¤ºrobotçŠ¶æ€ä¿¡æ¯")
    args = parser.parse_args()

    # åˆ›å»ºæ™ºèƒ½æ ¸å¿ƒ
    core = IntelligentCore(llm_url=args.llm)
    
    # æ˜¾ç¤ºrobotçŠ¶æ€ä¿¡æ¯
    if args.show_status:
        print("ğŸ¤– RobotçŠ¶æ€ä¿¡æ¯")
        print("=" * 50)
        print(f"ğŸ†” Robot ID: {args.robot_id}")
        print(f"ğŸŒ± æˆé•¿é˜¶æ®µ: {core.dialogue.stage}")
        print(f"ğŸ­ äººæ ¼é£æ ¼: {core.dialogue.personality.get_personality_style()}")
        print(f"ğŸ“Š äººæ ¼å‘é‡: {core.dialogue.personality.vector}")
        print(f"â­ ä¸»å¯¼ç‰¹è´¨: {core.dialogue.personality.get_dominant_traits()}")
        print(f"ğŸ’¾ è®°å¿†è®°å½•æ•°: {len(core.dialogue.memory.records)}")
        # å¯¼å…¥global_stateæ¨¡å—
        from ai_core import global_state
        print(f"ğŸ“ˆ äº¤äº’æ¬¡æ•°: {global_state.INTERACTION_COUNT}")
        print(f"ğŸµ éŸ³é¢‘æ—¶é•¿: {global_state.AUDIO_DATA_SECONDS:.1f}ç§’")
        print("=" * 50)
        print()

    # æ˜¾ç¤ºè®°å¿†ä¿¡æ¯
    if core.dialogue.memory.records:
        print("ğŸ’¾ æœ€è¿‘è®°å¿†è®°å½•")
        print("-" * 30)
        recent_memories = core.dialogue.memory.records[-3:]  # æ˜¾ç¤ºæœ€è¿‘3æ¡
        for i, memory in enumerate(recent_memories, 1):
            print(f"{i}. ç”¨æˆ·: {memory['user_text']}")
            print(f"   AI: {memory['ai_response']}")
            print(f"   æƒ…ç»ª: {memory['mood_tag']}")
            print(f"   æ—¶é—´: {memory['time']}")
            print()
    
    print("ğŸ”„ å¼€å§‹å¤„ç†ç”¨æˆ·è¾“å…¥...")
    print()

    user = UserInput(
        audio_path=args.audio,
        image_path=args.image,
        video_path=args.video,
        text=args.text,
        robot_id=args.robot_id,
        touch_zone=args.touch_zone,
    )

    reply = core.process(user)
    
    print("âœ… å¤„ç†å®Œæˆï¼")
    print("ğŸ“¤ è¾“å‡ºç»“æœ:")
    print(json.dumps(reply.as_dict(), ensure_ascii=False, indent=2))
    
    # æ˜¾ç¤ºå¤„ç†åçš„çŠ¶æ€å˜åŒ–
    if args.show_status:
        print("\nğŸ”„ å¤„ç†åçŠ¶æ€å˜åŒ–:")
        print("-" * 30)
        print(f"ğŸŒ± æˆé•¿é˜¶æ®µ: {core.dialogue.stage}")
        print(f"ğŸ­ äººæ ¼é£æ ¼: {core.dialogue.personality.get_personality_style()}")
        print(f"ğŸ“Š äººæ ¼å‘é‡: {core.dialogue.personality.vector}")
        print(f"ğŸ’¾ è®°å¿†è®°å½•æ•°: {len(core.dialogue.memory.records)}")
        print(f"ğŸ“ˆ äº¤äº’æ¬¡æ•°: {global_state.INTERACTION_COUNT}")


if __name__ == "__main__":
    main()