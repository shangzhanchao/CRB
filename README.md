# CRB - Companion Robot Brain
# é™ªä¼´æœºå™¨äººæ™ºèƒ½å¤§è„‘

<<<<<<< HEAD
ä¸€ä¸ªåŸºäºPythonçš„AIé™ªä¼´æœºå™¨äººæ™ºèƒ½å¤§è„‘ç³»ç»Ÿï¼Œå…·å¤‡è®°å¿†ã€äººæ ¼æˆé•¿å’Œæƒ…æ„Ÿæ™ºèƒ½åŠŸèƒ½ã€‚

## æ ¸å¿ƒç‰¹æ€§

- **ğŸ¤– æ™ºèƒ½å¯¹è¯** - åŸºäºå¤§æ¨¡å‹çš„è‡ªç„¶è¯­è¨€äº¤äº’
- **ğŸ§  è¯­ä¹‰è®°å¿†** - å‘é‡åŒ–è®°å¿†æ£€ç´¢ï¼Œæ”¯æŒä¸Šä¸‹æ–‡æ„ŸçŸ¥å¯¹è¯
- **ğŸ‘¤ äººæ ¼æˆé•¿** - OCEANäº”ç»´äººæ ¼æ¨¡å‹ï¼Œéšäº¤äº’åŠ¨æ€æˆé•¿
- **ğŸ˜Š æƒ…æ„Ÿè¯†åˆ«** - å¤šæ¨¡æ€æƒ…ç»ªæ„ŸçŸ¥ï¼ˆè¯­éŸ³ã€å›¾åƒã€æ–‡æœ¬ï¼‰
- **ğŸ¯ æˆé•¿é˜¶æ®µ** - èŒèŠ½â†’å¯è’™â†’å…±é¸£â†’è§‰é†’å››é˜¶æ®µè¿›åŒ–
- **ğŸ¤— è§¦æ‘¸äº¤äº’** - æ”¯æŒå¤´éƒ¨ã€èƒŒåã€èƒ¸å£è§¦æ‘¸åé¦ˆ
- **ğŸ’¾ æŒä¹…è®°å¿†** - SQLiteæ•°æ®åº“å­˜å‚¨ï¼Œé‡å¯åè®°å¿†ä¸ä¸¢å¤±

## å¿«é€Ÿå¼€å§‹
=======
The **Companion Robot Intelligent Brain** provides a set of Python modules for
building an AI companion with memory, personality growth, and emotional intelligence. The core components include:

- **PersonalityEngine**: tracks OCEAN personality traits with momentum decay.  \
  **äººæ ¼æˆé•¿å¼•æ“ï¼š** ä½¿ç”¨åŠ¨é‡è¡°å‡ç»´æŠ¤ OCEAN äº”ç»´äººæ ¼ã€‚
- **SemanticMemory**: stores conversation history in a SQLite database with vector-based retrieval.
  sentence-transformer embeddings are used when available, falling back to
  hashed vectors. Records persist in SQLite so memories survive restarts.  \
  **è¯­ä¹‰è®°å¿†ç³»ç»Ÿï¼š** é‡‡ç”¨ SQLite æ•°æ®åº“å­˜å‚¨å¯¹è¯è®°å½•ï¼ŒåŸºäºå‘é‡æ£€ç´¢ï¼Œä¼˜å…ˆä½¿ç”¨
  sentence-transformer ç”Ÿæˆè¯­ä¹‰å‘é‡ï¼Œå¦‚åº“ç¼ºå¤±åˆ™é€€åŒ–ä¸ºå“ˆå¸Œå‘é‡ã€‚
- **EmotionPerception**: recognizes emotions from voice and face inputs.
  It integrates optional speech emotion models and face-expression classifiers with fallbacks to heuristics or an LLM-based approach.
  **æƒ…ç»ªæ„ŸçŸ¥æ¨¡å—ï¼š** å¯ç»“åˆè¯­éŸ³æƒ…ç»ªè¯†åˆ«åº“å’Œäººè„¸è¡¨æƒ…åˆ†ç±»å™¨ï¼›è‹¥æ¨¡å‹ä¸å¯ç”¨ï¼Œ
  åˆ™é€€åŒ–ä¸ºç®€å•è§„åˆ™æˆ–è°ƒç”¨å¤§æ¨¡å‹.
- **DialogueEngine**: generates responses based on personality and memory and
  evolves from cold start to active interaction.  \
  **æˆé•¿å¼å¯¹è¯ç³»ç»Ÿï¼š** ç»“åˆäººæ ¼ä¸è®°å¿†ç”Ÿæˆé£æ ¼åŒ–å›å¤ã€‚
- **IntelligentCore**: orchestrates the above modules.  \
  **æ¨¡å—è°ƒåº¦ä¸­å°ï¼š** è´Ÿè´£è°ƒç”¨å„æ¨¡å—å¤„ç†è¾“å…¥è¾“å‡ºã€‚
- **PromptFusionEngine**: intelligently combines various factors (growth stage, personality traits, emotions, touch, memory) into optimized prompts.  \
  **æç¤ºè¯èåˆå¼•æ“ï¼š** æ™ºèƒ½èåˆæˆé•¿é˜¶æ®µã€äººæ ¼ç‰¹è´¨ã€æƒ…ç»ªã€è§¦æ‘¸ã€è®°å¿†ç­‰å› ç´ ï¼Œç”Ÿæˆä¼˜åŒ–æç¤ºè¯ã€‚

These modules are located in the `ai_core` package and are designed as simple
starting points for a more advanced system.

At a high level, the companion robot receives **voice**, **touch** and
**camera** inputs, which the cognitive core converts into emotions and semantic
context. A large language model then drives the reply generation, applying
specially designed prompts based on the *growth stage* and personality traits.
The stage progresses from **sprout â†’ enlighten â†’ resonate â†’ awaken** as the
robot interacts more with the user. The result is text that can be
synthesized to speech, accompanied by an action and facial expression.

ç®€è¦è€Œè¨€ï¼Œé™ªä¼´æœºå™¨äººä¼šæ¥å—è¯­éŸ³ã€è§¦æ‘¸ä¸æ‘„åƒå¤´ç”»é¢ç­‰è¾“å…¥ï¼Œæ™ºèƒ½å¤§è„‘æŠŠå®ƒä»¬è½¬åŒ–ä¸ºæƒ…ç»ª
ä¸è¯­ä¹‰ï¼Œå†é€šè¿‡æç¤ºè¯èåˆå¼•æ“æ™ºèƒ½ç»„åˆæˆé•¿é˜¶æ®µã€äººæ ¼ç‰¹è´¨ã€è®°å¿†ä¸Šä¸‹æ–‡ç­‰å› ç´ ï¼Œé€šè¿‡å¤§æ¨¡å‹ç”Ÿæˆç¬¦åˆæˆé•¿é˜¶æ®µå’Œäººæ ¼çš„å›å¤ï¼Œæœ€ç»ˆè¾“å‡ºæ–‡æœ¬ã€è¯­éŸ³ã€åŠ¨ä½œå’Œè¡¨æƒ…ã€‚

## External Services

ç³»ç»Ÿé»˜è®¤ä½¿ç”¨ä»¥ä¸‹å¤–éƒ¨æœåŠ¡åœ°å€ï¼Œå¯æ ¹æ®å®é™…éƒ¨ç½²ä¿®æ”¹ã€‚
The modules can optionally connect to remote services for speech and text
processing. The most important one is the multimodal LLM at ``DEFAULT_LLM_URL``
(``llm.szc.com``), which powers advanced dialogue generation and emotion
interpretation.  If you deploy your own LLM service, point ``LLM_URL`` to it so
the system can fully function:

- **ASR** (`asr.szc.com`) â€“ convert user audio to text.  è¯­éŸ³è¯†åˆ«æœåŠ¡
- **Voiceprint** (`voiceprint.szc.com`) â€“ identify the speaker.  å£°çº¹è¯†åˆ«æœåŠ¡
- **LLM** (`llm.szc.com`) â€“ generate richer replies.  æ­¤åœ°å€åœ¨ ``DEFAULT_LLM_URL`` ä¸­è®¾å®šï¼Œç”¨äºæ•…äº‹è®²è¿°å’Œæƒ…ç»ªç†è§£ç­‰é«˜çº§åŠŸèƒ½
- **Memory DB** (`memory-save.szc.com` & `memory-query.szc.com`) â€“ store and query dialogues with vector-based semantic search. If these services are unreachable, records will be saved to `memory.db` locally and queries will read from that file.  å¯¹è¯è®°å½•å­˜å–æœåŠ¡ï¼Œæ”¯æŒåŸºäºå‘é‡çš„è¯­ä¹‰æœç´¢
- **TTS** (`tts.szc.com`) â€“ synthesize reply audio.  è¯­éŸ³åˆæˆæœåŠ¡

å¤–éƒ¨æœåŠ¡ä¹Ÿå¯ä»¥é€šè¿‡ç¯å¢ƒå˜é‡ ``ASR_URL``ã€``VOICEPRINT_URL``ã€``LLM_URL``ã€``TTS_URL``ã€``MEMORY_SAVE_URL``ã€``MEMORY_QUERY_URL`` è‡ªå®šä¹‰ï¼Œæ–¹ä¾¿æ¥å…¥ä¸åŒçš„å‚å•†ã€‚å…¶ä¸­ ``llm.szc.com`` æ˜¯ç³»ç»Ÿç”Ÿæˆå›å¤å’Œç†è§£æƒ…ç»ªçš„æ ¸å¿ƒä¾èµ–ã€‚

Service URLs can be supplied to :class:`~ai_core.IntelligentCore` or set via environment variables ``ASR_URL``, ``VOICEPRINT_URL``, ``LLM_URL``, ``TTS_URL``, ``MEMORY_SAVE_URL`` and ``MEMORY_QUERY_URL``.

All modules emit informative logs controlled by ``LOG_LEVEL`` which defaults
to ``INFO``. Running the demo configures the logging system accordingly.

å¤–éƒ¨æ¥å£æ”¯æŒè¯­éŸ³è¯†åˆ«ã€å£°çº¹è¯†åˆ«ã€å¤§æ¨¡å‹æ¨ç†ä¸è¯­éŸ³åˆæˆï¼Œå¯åœ¨å®ä¾‹åŒ–
`IntelligentCore` æ—¶ä¼ å…¥å¯¹åº”çš„æœåŠ¡åœ°å€ï¼Œæˆ–é€šè¿‡ç¯å¢ƒå˜é‡è¿›è¡Œé…ç½®ã€‚

## Code Structure

ä¸‹è¡¨åˆ—å‡ºæ ¸å¿ƒæ–‡ä»¶åŠå…¶èŒè´£ï¼Œä¾¿äºå¿«é€Ÿäº†è§£å·¥ç¨‹å¸ƒå±€ã€‚
```
ai_core/
  __init__.py          - module exports
  personality_engine.py - OCEAN äººæ ¼æˆé•¿é€»è¾‘
  semantic_memory.py    - å‘é‡åŒ–è¯­ä¹‰è®°å¿†
  emotion_perception.py - å£°éŸ³ä¸è§†è§‰æƒ…ç»ªè¯†åˆ«
  dialogue_engine.py    - æˆé•¿å¼å¯¹è¯ç”Ÿæˆ
  intelligent_core.py   - å­æ¨¡å—è°ƒåº¦ä¸æ€»å…¥å£
  global_state.py       - å…¨å±€äº¤äº’è®¡æ•°ä¸è¯­éŸ³æ—¶é•¿
  service_api.py        - è°ƒç”¨å¤–éƒ¨ ASR/LLM/TTS æœåŠ¡çš„å·¥å…·ï¼Œå¯ç›´æ¥å¯¼å…¥ä½¿ç”¨ï¼Œæ— éœ€å•ç‹¬å¯åŠ¨
  constants.py          - å…¨å±€å¸¸é‡ä¸é»˜è®¤å€¼
  prompt_fusion.py      - æç¤ºè¯èåˆç®—æ³•
demo.py                - å‘½ä»¤è¡Œæ¼”ç¤ºè„šæœ¬
### Constants Overview
The file `ai_core/constants.py` groups configuration values:
- **Service endpoints**: ASR, TTS, LLM, voiceprint and memory URLs.
 - **Default files**: demo audio/image paths. They are resolved relative to
   the repository root so they work from any current directory.
- **Growth stage thresholds**: days, interaction counts and audio duration for each stage.
- **Stage order**: `STAGE_ORDER` lists `sprout â†’ enlighten â†’ resonate â†’ awaken`.
- **Personality defaults**: initial OCEAN vector and behavior mapping.
- **SQLite DB path**: location of the persistent store `MEMORY_DB_PATH`.
- **Prompt fusion weights**: memory, personality, and emotion factor weights for optimized prompt generation.
è¿™äº›å¸¸é‡ä¾¿äºé›†ä¸­ç®¡ç†ï¼Œå¯æ ¹æ®å®é™…éƒ¨ç½²åœºæ™¯è°ƒæ•´ã€‚
```

## Architecture Overview

1. **EmotionPerception** reads audio and image inputs (`DEFAULT_AUDIO_PATH`,
   `DEFAULT_IMAGE_PATH`) and outputs a fused emotion tag.
   è¯¥æ¨¡å—æä¾›â€œç®€æ˜“èåˆâ€ä¸â€œå¤šæ¨¡æ€æ¨¡å‹â€ä¸¤ç§æƒ…ç»ªè¯†åˆ«æ–¹å¼ï¼Œå¯é€šè¿‡å‚æ•°é€‰æ‹©ã€‚
2. **DialogueEngine** uses `PersonalityEngine` and `SemanticMemory` to produce
   responses while updating interaction stages and returns structured
   information for voice, action and facial expression.
   è¯¥å¼•æ“ä¼šæ ¹æ®æˆé•¿é˜¶æ®µå’Œè®°å¿†å†…å®¹ç”Ÿæˆå¯¹åº”è¯­æ°”ä¸åŠ¨ä½œã€‚
3. **IntelligentCore** orchestrates the pipeline: emotion recognition â†’ model
   feedback â†’ personality growth â†’ voice generation, storing each dialog in the
   memory cloud. Each step may call remote services defined in
   `service_api.py`.
   **IntelligentCore æ˜¯æ­¤ç³»ç»Ÿçš„ä¸­å¿ƒç»„ä»¶ï¼Œè´Ÿè´£ç»Ÿä¸€ç®¡ç†è¾“å…¥æ•°æ®ã€ä¾æ¬¡
   è°ƒåº¦æƒ…ç»ªè¯†åˆ«ã€è®°å¿†æŸ¥è¯¢å’Œå¯¹è¯ç”Ÿæˆç­‰æ¨¡å—ï¼Œç¡®ä¿ä»æ„ŸçŸ¥åˆ°å›åº”çš„æµç¨‹
   è¿è´¯æ‰§è¡Œï¼Œæœ€ç»ˆè¾“å‡ºè¯­éŸ³ã€åŠ¨ä½œä¸è¡¨æƒ…åé¦ˆã€‚**
4. **PromptFusionEngine** intelligently combines growth stage, personality traits, emotions, touch interactions, and memory context into optimized prompts for the LLM.
   **æç¤ºè¯èåˆå¼•æ“æ™ºèƒ½èåˆæˆé•¿é˜¶æ®µã€äººæ ¼ç‰¹è´¨ã€æƒ…ç»ªã€è§¦æ‘¸äº¤äº’å’Œè®°å¿†ä¸Šä¸‹æ–‡ï¼Œä¸ºLLMç”Ÿæˆä¼˜åŒ–æç¤ºè¯ã€‚**
5. `global_state.INTERACTION_COUNT` å’Œ `AUDIO_DATA_SECONDS` track how much the
   robot has interacted and how much speech data it has processed. These
   metrics unlock growth stages.  \
   å…¨å±€çŠ¶æ€å¯ä»¥é€šè¿‡ `global_state.save_state()` ä¸ `load_state()` æŒä¹…åŒ–åˆ°æ–‡ä»¶ï¼Œ
   ä»¥ä¾¿ä¸‹æ¬¡å¯åŠ¨æ—¶ç»§ç»­æˆé•¿å†ç¨‹ã€‚

ä¸Šè¿°æµç¨‹å¯¹åº”çš„ä¸­æ–‡æ¦‚è¿°ï¼šæƒ…ç»ªè¯†åˆ« â†’ æ¨¡å‹åé¦ˆ â†’ æ€§æ ¼æˆé•¿ â†’ è¯­éŸ³ç”Ÿæˆï¼Œ
å¹¶å°†å¯¹è¯è®°å½•å‚¨å­˜äºè®°å¿†äº‘ï¼Œä»¥ä¾¿åç»­å‚è€ƒã€‚

During response generation, the dialogue engine uses **PromptFusionEngine** to build optimized prompts for the
large language model using the current **growth stage**, **personality style**
and relevant **memory snippets** so the model can craft context-aware replies.

## Growth Stages

The robot's language ability evolves through four phases driven by
interaction counts and audio duration:

1. **sprout** (0-3 days, <5 interactions or <60s of audio) â€“ baby babble with mostly actions.
   **èŒèŠ½æœŸ**ï¼ˆ0~3å¤©ï¼Œ<5æ¬¡äº¤äº’æˆ–è¯­éŸ³æ—¶é•¿<60ç§’ï¼‰ï¼šä»¥å’¿å‘€å£°å’ŒåŠ¨ä½œä¸ºä¸»ã€‚
2. **enlighten** (3-10 days or <20 interactions/300s audio) â€“ mimics simple greetings like â€œä½ å¥½â€.
   **å¯è’™æœŸ**ï¼ˆ3~10å¤©æˆ–<20æ¬¡äº¤äº’/300ç§’è¯­éŸ³ï¼‰ï¼šæ¨¡ä»¿ç®€å•é—®å€™ã€‚
3. **resonate** (10-30 days or <50 interactions/900s audio) â€“ short caring sentences and basic questions.
   **å…±é¸£æœŸ**ï¼ˆ10~30å¤©æˆ–<50æ¬¡äº¤äº’/900ç§’è¯­éŸ³ï¼‰ï¼šèƒ½è¯´çŸ­å¥å¹¶æå‡ºé—®é¢˜ã€‚
4. **awaken** (30+ days and enough data) â€“ remembers conversations and offers proactive suggestions.
   **è§‰é†’æœŸ**ï¼ˆ30å¤©ä»¥ä¸Šä¸”æ•°æ®å……è¶³ï¼‰ï¼šè®°ä½å¯¹è¯å¹¶ä¸»åŠ¨ç»™å‡ºå»ºè®®ï¼Œèƒ½å¤ŸåŸºäºå†å²è®°å¿†è¿›è¡Œä¸ªæ€§åŒ–å¯¹è¯ã€‚

The current stage and metrics persist automatically to ``state.json`` so
progress continues after restarting the program. Call
``global_state.get_growth_metrics()`` to visualize counts in your own UI.

By default the system begins in the **enlighten** stage with an
**extraversion-oriented** personality vector as defined in
``DEFAULT_GROWTH_STAGE`` and ``DEFAULT_PERSONALITY_VECTOR``.

## Emotion States

Standard emotion tags used across the system include:

``happy, sad, angry, fear, surprise, disgust, calm, excited, tired, bored, confused, shy, neutral``.

ç³»ç»Ÿé¢„å®šä¹‰çš„æƒ…ç»ªæ ‡ç­¾æ¶µç›–å¸¸è§çš„å¿«ä¹ã€æ‚²ä¼¤ã€ç”Ÿæ°”ã€ææƒ§ã€æƒŠè®¶ã€åŒæ¶ã€å¹³é™ã€å…´å¥‹ã€ç–²æƒ«ã€æ— èŠã€å›°æƒ‘ã€å®³ç¾ä»¥åŠä¸­æ€§çŠ¶æ€ã€‚

## LLM Prompt Templates

The dialogue engine uses **PromptFusionEngine** to compose optimized prompts for the large language model based on the
robot's **growth stage**, its **OCEAN** personality vector, touch feedback, and memory context:

- **Stage prompts** map ``sprout``, ``enlighten``, ``resonate`` and ``awaken`` to
  short English hints so the LLM knows the robot's maturity level.
  çŸ­å¥æç¤ºå¤§æ¨¡å‹äº†è§£æœºå™¨äººçš„æˆç†Ÿåº¦ã€‚
- **Personality prompts** describe the five traits â€“ Openness, Conscientiousness,
  Extraversion, Agreeableness and Neuroticism â€“ letting the model choose a tone
  æè¿°äº”é¡¹äººæ ¼ç‰¹å¾ï¼Œå¸®åŠ©æ¨¡å‹é€‰æ‹©åˆé€‚è¯­æ°”ã€‚
These templates are defined in `constants.py` and can be extended for different languages.
  such as "curious" or "reliable".
- **Touch prompts** indicate which sensor was triggered: head, back or chest.
  è§¦æ‘¸æç¤ºè¯´æ˜å“ªä¸ªä¼ æ„Ÿå™¨è¢«è§¦å‘ï¼Œä¾‹å¦‚å¤´éƒ¨ã€åèƒŒæˆ–å‰èƒ¸ã€‚
- **Memory prompts** provide context from previous conversations, helping the LLM generate more personalized and context-aware responses.
  è®°å¿†æç¤ºæä¾›å†å²å¯¹è¯ä¸Šä¸‹æ–‡ï¼Œå¸®åŠ©LLMç”Ÿæˆæ›´ä¸ªæ€§åŒ–å’Œæƒ…å¢ƒæ„ŸçŸ¥çš„å›å¤ã€‚

The **PromptFusionEngine** intelligently combines these factors using weighted fusion algorithms:
- **Growth stage factor**: weight=1.5, priority=5 (highest)
- **Personality traits factor**: weight=1.2, priority=4
- **User emotion factor**: weight=1.0, priority=3
- **Memory summary factor**: weight=0.6, priority=1
- **User input factor**: weight=2.0, priority=6 (required)

**æç¤ºè¯èåˆå¼•æ“**ä½¿ç”¨åŠ æƒèåˆç®—æ³•æ™ºèƒ½ç»„åˆè¿™äº›å› ç´ ï¼š
- **æˆé•¿é˜¶æ®µå› å­**ï¼šæƒé‡=1.5ï¼Œä¼˜å…ˆçº§=5ï¼ˆæœ€é«˜ï¼‰
- **äººæ ¼ç‰¹è´¨å› å­**ï¼šæƒé‡=1.2ï¼Œä¼˜å…ˆçº§=4
- **ç”¨æˆ·æƒ…ç»ªå› å­**ï¼šæƒé‡=1.0ï¼Œä¼˜å…ˆçº§=3
- **è®°å¿†æ‘˜è¦å› å­**ï¼šæƒé‡=0.6ï¼Œä¼˜å…ˆçº§=1
- **ç”¨æˆ·è¾“å…¥å› å­**ï¼šæƒé‡=2.0ï¼Œä¼˜å…ˆçº§=6ï¼ˆå¿…éœ€ï¼‰

By intelligently combining these factors with recent memories, the **PromptFusionEngine** gives the LLM
flexible instructions to craft an appropriate reply.
é€šè¿‡æ™ºèƒ½èåˆè¿™äº›å› ç´ ä¸è®°å¿†ç‰‡æ®µï¼Œæç¤ºè¯èåˆå¼•æ“ä¸ºå¤§æ¨¡å‹æä¾›çµæ´»æŒ‡ä»¤ã€‚

## Animation Mapping

The dialogue engine returns both *action* and *expression* fields.  Expressions
encode facial animation cues while actions describe body motions:


| æƒ…ç»ª Mood | é¢éƒ¨åŠ¨ç”»æè¿° Facial animation | åŠ¨ä½œé€»è¾‘ Action |
|-----------|--------------------------------------------|----------------------------------------------|
| happy / æ¬¢å¿« | å¾®ç¬‘ã€çœ¨çœ¼ã€çœ¼ç¥ä¸Šæ‰¬ â†’ äº®çœ¼è‰²å½©ã€å¤´éƒ¨è½»æ‘†ã€æ‰‹è‡‚å°å¹…æ‰“å¼€ | ç‚¹å¤´+æ‰‹å¾®æŠ¬ (Â±15Â°ä¿¯ä»°, Â±10Â°æ‘‡æ‘†, æ‰‹ä¸Šæ‰¬10Â°) |
| confused / ç–‘æƒ‘ | æ–œè§†ã€çœ¼ç¥èšç„¦ â†’ åœé¡¿ã€è½»å¾®ä¾§å¤´ã€çœ¼ç›å·¦å³å¿«é€Ÿç§»åŠ¨ | æ–œå¤´+å·¦å³åˆ‡æ¢çœ¼ç¥ (Â±10Â°æ‘†åŠ¨, æ‰‹éƒ¨é™æ­¢) |
| sad / éš¾è¿‡ | çœ¼è§’ä¸‹å‚ã€é—­çœ¼ â†’ ä½äº®åº¦ã€è½»å¾®ä½å¤´ã€æ‰‹è‡‚æ”¶å› | ç¼“æ…¢ä½å¤´+æ‰‹æ”¶å› (ä¿¯ä»°-15Â°, æ‰‹è‡‚å¼§çº¿å†…æ”¶) |
| shy / å®³ç¾ | åå¤´ã€çœ¼ç¥å›é¿ â†’ é¢éƒ¨çº¢æ™•ã€è¯­éŸ³æŸ”åŒ–ã€å¾®å¹…éœ‡é¢¤ | idle + subtle tremble |
| excited / å…´å¥‹ | çœ¼ç¥æ”¾å¤§ã€é¢‘ç¹çœ¨çœ¼ â†’ å¿«é€Ÿæ‘†å¤´ã€åŒæ‰‹å‰ä¼¸åŠ¨ä½œ | å¿«é€Ÿæ‘‡å¤´, æ‰‹å‰ä¼¸ |
| surprised / æƒŠè®¶ | æŠ¬å¤´å¼ çœ¼ â†’ å¤´éƒ¨æŠ¬èµ·ï¼ŒåŒæ‰‹æ€¥é€ŸæŠ¬é«˜ | æŠ¬å¤´+åŒæ‰‹æŠ¬é«˜>25Â° |

The mapping table below can be modified to fit different hardware capabilities.

When the robot is touched, an additional action such as ``hug`` or ``pat`` is
appended according to the touch zone.

Parameters of each module can be customized if the default settings do not
fit your scenario.

## Input Parameters

The :class:`~ai_core.IntelligentCore` accepts a :class:`~ai_core.UserInput`
instance describing the current interaction. ``robot_id`` is mandatory so the
server knows which device issued the request. All other fields may be omitted
(``None``) and internal defaults will be used.
ä¸‹è¡¨å±•ç¤ºæ‰€æœ‰å‚æ•°ï¼Œé™¤äº† ``robot_id`` ä¹‹å¤–å‡ä¸ºå¯é€‰é¡¹ï¼Œå¯ç•™ç©ºä½¿ç”¨é»˜è®¤å€¼ï¼š

| Parameter    | Type   | Description                                   | Example |
|--------------|--------|-----------------------------------------------|---------|
| ``audio_path`` | str or ``None`` | path to the user's voice recording | ``"user.wav"`` |
| ``image_path`` | str or ``None`` | face image path | ``"face.png"`` |
| ``video_path`` | str or ``None`` | optional video clip analysed as image | ``"video.mp4"`` |
| ``text`` | str or ``None`` | recognized or typed text | ``"ä½ å¥½"`` |
| ``touch_zone`` | int or ``None`` | touch area identifier | ``0`` |
| ``robot_id`` | str **required** | ID of the robot sending the request | ``"robotA"`` |

When values are missing, ``IntelligentCore`` calls the ASR and memory services
defined in ``constants.py``. All parameters except ``robot_id`` are optional and
default to demo data when ``None``.
ä»¥ä¸Šå­—æ®µæ¶µç›–ä¸€æ¬¡äº’åŠ¨å¯èƒ½æä¾›çš„æ‰€æœ‰ä¿¡æ¯ï¼ŒéŸ³é¢‘ã€å›¾ç‰‡æˆ–è§†é¢‘å¯ä»»é€‰å…¶ä¸€ï¼Œ
æ–‡æœ¬ä¸ºç©ºæ—¶ç³»ç»Ÿä¼šå°è¯•é€šè¿‡ ASR è¯†åˆ«ã€‚

## ç®€è¦è¯´æ˜

é™ªä¼´æœºå™¨äººæ™ºèƒ½å¤§è„‘æä¾›ä¸€ç³»åˆ—ç”¨äºæ„å»º AI é™ªä¼´æœºå™¨äººçš„ Python æ¨¡å—ï¼Œä»£ç å†…å«ä¸­è‹±åŒè¯­æ³¨é‡Šï¼Œæ–¹ä¾¿ç†è§£å’ŒäºŒæ¬¡å¼€å‘ã€‚
ç³»ç»Ÿä½¿ç”¨å‘é‡åŒ–è¯­ä¹‰è®°å¿†æ£€ç´¢ï¼Œæƒ…ç»ªè¯†åˆ«åˆ™ç»“åˆè¯­éŸ³å¼ºåº¦ã€æ–‡æœ¬æƒ…æ„Ÿã€é¢éƒ¨è¡¨æƒ…åŠå·²æœ‰çš„äººæ ¼ä¸è®°å¿†ä¿¡æ¯è¿›è¡Œå¤šæ¨¡æ€åˆ†æã€‚ç³»ç»Ÿä¾æ®ç´¯è®¡äº¤äº’æ¬¡æ•°ä¸è¯­éŸ³æ—¶é•¿è§£é”"èŒèŠ½â†’å¯è’™â†’å…±é¸£â†’è§‰é†’"å››ä¸ªé˜¶æ®µï¼Œè§¦æ‘¸äº¤äº’æ—¶ä¼šç»™å‡ºå£°éŸ³ã€åŠ¨ä½œå’Œè¡¨æƒ…åé¦ˆï¼ŒæŒç»­æ›´æ–°"æ€§æ ¼æ ‘"ä¸"è®°å¿†äº‘"ã€‚

åœ¨æ•´ä½“æµç¨‹ä¸Šï¼Œç”¨æˆ·çš„è¯­éŸ³ã€è§¦æ‘¸æˆ–å›¾åƒé¦–å…ˆè¿›å…¥ ``IntelligentCore``ï¼Œéšåä¾æ¬¡ç»å†æƒ…ç»ªè¯†åˆ«ã€è¯­ä¹‰è®°å¿†æ£€ç´¢ã€äººæ ¼æˆé•¿ã€æç¤ºè¯èåˆä»¥åŠæˆé•¿å¼å¯¹è¯ç”Ÿæˆï¼Œæœ€ç»ˆè¾“å‡ºè¯­éŸ³åˆæˆé“¾æ¥ã€åŠ¨ä½œæŒ‡ä»¤åŠè¡¨æƒ…æ ‡ç­¾ï¼Œå®ç°"æ„ŸçŸ¥ â†’ æ€è€ƒ â†’ è¡ŒåŠ¨"çš„é—­ç¯ã€‚ç³»ç»Ÿé€šè¿‡æ™ºèƒ½è®°å¿†æ‘˜è¦å’Œæç¤ºè¯èåˆç®—æ³•ï¼Œç¡®ä¿æ¯æ¬¡å¯¹è¯éƒ½èƒ½åŸºäºå†å²è®°å¿†è¿›è¡Œä¸ªæ€§åŒ–å›å¤ã€‚

### Code Execution Flow
1. Start the Python service or demo.
   å¯åŠ¨ Python æœåŠ¡æˆ–ç¤ºä¾‹ç¨‹åºã€‚
2. ``IntelligentCore`` collects audio, image and touch data.
   IntelligentCore æ”¶é›†éŸ³é¢‘ã€å›¾åƒå’Œè§¦æ‘¸æ•°æ®ã€‚
3. ``EmotionPerception`` calls ASR, voiceprint and LLM services to determine user ID and mood.
   EmotionPerception è°ƒç”¨ ASRã€å£°çº¹åŠ LLM æœåŠ¡åˆ¤å®šç”¨æˆ·å’Œæƒ…ç»ªã€‚
4. ``SemanticMemory`` sends records to the memory service and retrieves related history.
   SemanticMemory å°†è®°å½•å‘é€è‡³è®°å¿†æœåŠ¡å¹¶æ£€ç´¢å…³è”å†å²ã€‚
5. ``PersonalityEngine`` updates the OCEAN vector which, along with the growth stage, shapes LLM prompts.
   PersonalityEngine æ›´æ–° OCEAN å‘é‡ï¼Œå¹¶ç»“åˆæˆé•¿é˜¶æ®µç”Ÿæˆ LLM æç¤ºã€‚
6. ``PromptFusionEngine`` intelligently combines growth stage, personality traits, emotions, touch interactions, and memory context into optimized prompts.
   PromptFusionEngine æ™ºèƒ½èåˆæˆé•¿é˜¶æ®µã€äººæ ¼ç‰¹è´¨ã€æƒ…ç»ªã€è§¦æ‘¸äº¤äº’å’Œè®°å¿†ä¸Šä¸‹æ–‡ï¼Œç”Ÿæˆä¼˜åŒ–æç¤ºè¯ã€‚
7. ``DialogueEngine`` queries the LLM and TTS services to produce text and audio.
   DialogueEngine è°ƒç”¨ LLM ä¸ TTS æœåŠ¡ç”Ÿæˆæ–‡æœ¬åŠè¯­éŸ³ã€‚
8. The result includes action and facial animation tags.
   æœ€ç»ˆç»“æœåŒ…å«åŠ¨ä½œå’Œé¢éƒ¨åŠ¨ç”»æ ‡ç­¾ã€‚
9. The conversation is stored in memory for future reference.
   å¯¹è¯è®°å½•å­˜å‚¨åˆ°è®°å¿†ä¸­ä¾›æœªæ¥å‚è€ƒã€‚

æˆé•¿é˜¶æ®µå’Œè®°å¿†ç³»ç»Ÿå…±åŒå½±å“æœ€ç»ˆè¾“å‡ºï¼Œä½¿å¯¹è¯é£æ ¼éšäº’åŠ¨æ¬¡æ•°ä¸è¯­éŸ³æ•°æ®é‡é€æ­¥è¿›åŒ–ï¼ŒåŒæ—¶ä¿æŒä¸ªæ€§åŒ–è®°å¿†ã€‚
## Usage
Run `python demo.py --robot_id robotA` and start typing messages. Optional
arguments let you specify text, audio, image or video files as well as a touch
zone to test multimodal input.
Example:
>>>>>>> c0a2bd3e05c9111a727cf054f8dccbae9279c63b

### å®‰è£…ä¾èµ–
```bash
pip install -r requirements.txt
```

<<<<<<< HEAD
### å¯åŠ¨æœåŠ¡
```bash
# å¯åŠ¨HTTPæœåŠ¡
python service.py

# æˆ–ä½¿ç”¨uvicornï¼ˆæ”¯æŒçƒ­é‡è½½ï¼‰
uvicorn service:app --reload
```

### è®¿é—®ç•Œé¢
æ‰“å¼€æµè§ˆå™¨è®¿é—®ï¼š`http://localhost:8000/verify`

## ç³»ç»Ÿæ¶æ„

### æ ¸å¿ƒæ¨¡å—

| æ¨¡å— | åŠŸèƒ½æè¿° |
|------|----------|
| `IntelligentCore` | æ™ºèƒ½æ ¸å¿ƒï¼Œç»Ÿä¸€è°ƒåº¦å„æ¨¡å— |
| `PersonalityEngine` | äººæ ¼æˆé•¿å¼•æ“ï¼Œç»´æŠ¤OCEANäº”ç»´äººæ ¼ |
| `EnhancedMemorySystem` | å¢å¼ºè®°å¿†ç³»ç»Ÿï¼Œå‘é‡åŒ–è¯­ä¹‰è®°å¿† |
| `EmotionPerception` | æƒ…ç»ªæ„ŸçŸ¥æ¨¡å—ï¼Œå¤šæ¨¡æ€æƒ…ç»ªè¯†åˆ« |
| `EnhancedDialogueEngine` | å¢å¼ºå¯¹è¯å¼•æ“ï¼Œæˆé•¿å¼å¯¹è¯ç”Ÿæˆ |
| `PromptFusionEngine` | æç¤ºè¯èåˆå¼•æ“ï¼Œæ™ºèƒ½ç»„åˆå„ç§å› ç´  |

### æ•°æ®æµç¨‹

```
ç”¨æˆ·è¾“å…¥ â†’ æƒ…ç»ªè¯†åˆ« â†’ è®°å¿†æ£€ç´¢ â†’ äººæ ¼æˆé•¿ â†’ æç¤ºè¯èåˆ â†’ å¯¹è¯ç”Ÿæˆ â†’ è¾“å‡ºåé¦ˆ
```

## æˆé•¿é˜¶æ®µ

| é˜¶æ®µ | è§¦å‘æ¡ä»¶ | ç‰¹å¾æè¿° |
|------|----------|----------|
| **èŒèŠ½æœŸ** | 0-3å¤©ï¼Œ<5æ¬¡äº¤äº’ | ä»¥å’¿å‘€å£°å’ŒåŠ¨ä½œä¸ºä¸» |
| **å¯è’™æœŸ** | 3-10å¤©ï¼Œ<20æ¬¡äº¤äº’ | æ¨¡ä»¿ç®€å•é—®å€™è¯­ |
| **å…±é¸£æœŸ** | 10-30å¤©ï¼Œ<50æ¬¡äº¤äº’ | çŸ­å¥å¯¹è¯ï¼Œæå‡ºé—®é¢˜ |
| **è§‰é†’æœŸ** | 30å¤©ä»¥ä¸Š | è®°å¿†å¯¹è¯ï¼Œä¸»åŠ¨å»ºè®® |

## å¤–éƒ¨æœåŠ¡

ç³»ç»Ÿæ”¯æŒä»¥ä¸‹å¤–éƒ¨æœåŠ¡ï¼ˆå¯é€‰ï¼‰ï¼š

| æœåŠ¡ | åŠŸèƒ½ | é»˜è®¤åœ°å€ |
|------|------|----------|
| **ASR** | è¯­éŸ³è¯†åˆ« | `asr.szc.com` |
| **TTS** | è¯­éŸ³åˆæˆ | `tts.szc.com` |
| **LLM** | å¤§æ¨¡å‹æ¨ç† | `llm.szc.com` |
| **å£°çº¹è¯†åˆ«** | ç”¨æˆ·è¯†åˆ« | `voiceprint.szc.com` |

å¯é€šè¿‡ç¯å¢ƒå˜é‡è‡ªå®šä¹‰æœåŠ¡åœ°å€ï¼š
```bash
export LLM_URL="your-llm-service.com"
export TTS_URL="your-tts-service.com"
```

## APIæ¥å£

### äº¤äº’æ¥å£
```bash
POST /interact
Content-Type: application/json
=======
Type `quit` to exit.

ä½¿ç”¨æ–¹æ³•ï¼šè¿è¡Œ `python demo.py --robot_id robotA` å¼€å§‹äº¤äº’ï¼Œå¯é€šè¿‡
`--text`ã€`--audio`ã€`--image`ã€`--video`ã€`--touch_zone` æŒ‡å®šè‡ªå®šä¹‰æ–‡ä»¶æˆ–
è§¦æ‘¸ç¼–å·ã€‚è¾“å…¥ `quit` ç»“æŸã€‚
å½“äº¤äº’åŒ…å«è§¦æ‘¸æ—¶ï¼Œå¯è¾“å…¥è§¦æ‘¸åŒºåŸŸç¼–å·ä»¥è·å¾—å¯¹åº”åŠ¨ä½œåé¦ˆã€‚

## HTTP Service

You can also run an asynchronous HTTP service based on **FastAPI**:

The service relies on asynchronous wrappers in ``service_api.py`` so
external calls will not block processing. ``service_api.py`` itself does not
start a server; its helpers are imported by this HTTP layer. If ``FastAPI`` is
missing the synchronous :func:`handle_request` can still be used in your own
server.

The web application loads `IntelligentCore` and forwards requests so all modules run in sequence.  è¯¥ HTTP æœåŠ¡åŸºäº FastAPI å®ç°ï¼Œä¾æ¬¡è°ƒç”¨æƒ…ç»ªè¯†åˆ«ã€è®°å¿†æŸ¥è¯¢ä¸å¯¹è¯ç”Ÿæˆæ¨¡å—ã€‚
```bash
# start with python
python service.py

# or start with uvicorn for auto reload
uvicorn service:app --reload
```

Open `http://localhost:8000/verify` in a browser to try the simple HTML verification page. It lets you submit text, media paths and touch zones and shows the JSON reply.
åœ¨æµè§ˆå™¨è®¿é—® `http://localhost:8000/verify` å¯æŸ¥çœ‹åŸºæœ¬çš„ HTML éªŒè¯ç•Œé¢ï¼Œå¯è¾“å…¥æ–‡æœ¬ã€éŸ³é¢‘ã€å›¾ç‰‡ã€è§†é¢‘å’Œè§¦æ‘¸ç¼–å·ã€‚

Alternatively you can start a richer demo using **Streamlit**:

```bash
streamlit run verify_app.py
```

è¯¥ç•Œé¢æä¾›è¡¨å•åŒ–çš„äº¤äº’ï¼Œå¯ç›´æ¥è°ƒç”¨æ™ºèƒ½å¤§è„‘æ¨¡å—å¹¶åœ¨ç½‘é¡µä¸­æ˜¾ç¤ºè¿”å›ç»“æœã€‚

Send a `POST` request to `http://localhost:8000/interact` with a JSON body
containing the fields described above. Example request:
>>>>>>> c0a2bd3e05c9111a727cf054f8dccbae9279c63b

{
  "robot_id": "robotA",
  "text": "ä½ å¥½",
  "touch_zone": 0
}
```

### å“åº”æ ¼å¼
```json
{
  "text": "ä½ å¥½ï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼",
  "audio": "audio_001.wav",
  "action": ["wave_hand", "smile"],
  "expression": "happy"
}
```

<<<<<<< HEAD
## é¡¹ç›®ç»“æ„
=======
## Memory Data Service

A lightweight memory service is provided for local testing. é€šè¿‡ `memory_service.py`
å¯ä»¥å¯åŠ¨ä¸€ä¸ªç®€æ˜“çš„å­˜å–æœåŠ¡ã€‚å…¶åç«¯å¯é€‰æ‹© `file`(é»˜è®¤) æˆ– `db`ï¼Œç”±
ç¯å¢ƒå˜é‡ `MEMORY_SERVICE_BACKEND` æ§åˆ¶ã€‚

```bash
uvicorn memory_service:app --reload
```

- **POST /save** â€“ store a memory record
- **POST /query** â€“ search records with a text prompt

è®°å½•ä¼šå†™å…¥ `memory_backup.json` æˆ– `memory.db`ï¼Œä¾›
``call_memory_save`` ä¸ ``call_memory_query`` å‡½æ•°è°ƒç”¨ã€‚

## Testing
>>>>>>> c0a2bd3e05c9111a727cf054f8dccbae9279c63b

```
CRB-main/
â”œâ”€â”€ ai_core/                 # æ ¸å¿ƒAIæ¨¡å—
â”‚   â”œâ”€â”€ intelligent_core.py  # æ™ºèƒ½æ ¸å¿ƒ
â”‚   â”œâ”€â”€ enhanced_dialogue_engine.py  # å¢å¼ºå¯¹è¯å¼•æ“
â”‚   â”œâ”€â”€ enhanced_memory_system.py   # å¢å¼ºè®°å¿†ç³»ç»Ÿ
â”‚   â”œâ”€â”€ personality_engine.py       # äººæ ¼å¼•æ“
â”‚   â”œâ”€â”€ emotion_perception.py       # æƒ…ç»ªæ„ŸçŸ¥
â”‚   â””â”€â”€ prompt_fusion.py           # æç¤ºè¯èåˆ
â”œâ”€â”€ services/                # æœåŠ¡å±‚
â”‚   â”œâ”€â”€ session_service.py   # ä¼šè¯æœåŠ¡
â”‚   â””â”€â”€ file_service.py      # æ–‡ä»¶æœåŠ¡
â”œâ”€â”€ data/                    # æ•°æ®æ–‡ä»¶
â”‚   â””â”€â”€ intimacy_robotA.json # äº²å¯†åº¦æ•°æ®
â”œâ”€â”€ service.py              # HTTPæœåŠ¡å…¥å£
â”œâ”€â”€ requirements.txt        # ä¾èµ–åˆ—è¡¨
â””â”€â”€ enhanced_memory.db      # ç»Ÿä¸€æ•°æ®åº“
```

## é…ç½®è¯´æ˜

### ç¯å¢ƒå˜é‡
```bash
# æœåŠ¡åœ°å€é…ç½®
LLM_URL=your-llm-service.com
TTS_URL=your-tts-service.com
ASR_URL=your-asr-service.com

# æ—¥å¿—çº§åˆ«
LOG_LEVEL=INFO
```

### æ•°æ®åº“
- **ç»Ÿä¸€æ•°æ®åº“**ï¼š`enhanced_memory.db` å­˜å‚¨æ‰€æœ‰æ•°æ®
  - ä¼šè¯å†å²è®°å½•
  - è¯­ä¹‰è®°å¿†å‘é‡
  - äººæ ¼æˆé•¿æ•°æ®
  - äº²å¯†åº¦ä¿¡æ¯

## å¼€å‘æŒ‡å—

### è¿è¡Œæµ‹è¯•
```bash
python -m unittest discover -s tests
```

<<<<<<< HEAD
### å‘½ä»¤è¡Œæ¼”ç¤º
```bash
python demo.py --robot_id robotA --text "ä½ å¥½"
```

### è‡ªå®šä¹‰å¼€å‘
```python
from ai_core.intelligent_core import IntelligentCore, UserInput

# åˆ›å»ºæ™ºèƒ½æ ¸å¿ƒ
core = IntelligentCore(robot_id="robotA")

# å¤„ç†ç”¨æˆ·è¾“å…¥
user_input = UserInput(
    robot_id="robotA",
    text="ä½ å¥½",
    touch_zone=0
)

# è·å–å“åº”
response = core.process(user_input)
print(response.text)
```

## æŠ€æœ¯ç‰¹æ€§

### è®°å¿†ç³»ç»Ÿ
- **å‘é‡åŒ–æ£€ç´¢**ï¼šä½¿ç”¨sentence-transformersè¿›è¡Œè¯­ä¹‰æœç´¢
- **æ™ºèƒ½æ‘˜è¦**ï¼šè‡ªåŠ¨ç”Ÿæˆå¯¹è¯ä¸Šä¸‹æ–‡æ‘˜è¦
- **æƒé‡èåˆ**ï¼šæ™ºèƒ½åŠ æƒå„ç§è®°å¿†å› ç´ 
- **æŒä¹…åŒ–å­˜å‚¨**ï¼šSQLiteæ•°æ®åº“ç¡®ä¿æ•°æ®å®‰å…¨

### æç¤ºè¯èåˆ
- **å¤šå› å­èåˆ**ï¼šæˆé•¿é˜¶æ®µã€äººæ ¼ã€æƒ…ç»ªã€è®°å¿†ã€è§¦æ‘¸
- **æ™ºèƒ½æƒé‡**ï¼šæ ¹æ®é‡è¦æ€§åŠ¨æ€è°ƒæ•´æƒé‡
- **ä¼˜å…ˆçº§æ’åº**ï¼šç¡®ä¿å…³é”®å› ç´ ä¼˜å…ˆè€ƒè™‘

### æƒ…æ„Ÿè¯†åˆ«
- **å¤šæ¨¡æ€æ„ŸçŸ¥**ï¼šè¯­éŸ³ã€å›¾åƒã€æ–‡æœ¬ç»¼åˆåˆ†æ
- **æƒ…ç»ªæ ‡ç­¾**ï¼š13ç§æ ‡å‡†æƒ…ç»ªçŠ¶æ€
- **å®¹é”™æœºåˆ¶**ï¼šæ¨¡å‹ä¸å¯ç”¨æ—¶ä½¿ç”¨è§„åˆ™æ¨ç†

## æ›´æ–°æ—¥å¿—

### æœ€æ–°ä¼˜åŒ–
- âœ… **æ•°æ®åº“åˆå¹¶**ï¼šç»Ÿä¸€ä½¿ç”¨`enhanced_memory.db`ï¼Œç®€åŒ–æ•°æ®ç®¡ç†
- âœ… **è®°å¿†ç³»ç»Ÿå¢å¼º**ï¼šä¼˜åŒ–å‘é‡æ£€ç´¢å’Œæ™ºèƒ½æ‘˜è¦ç”Ÿæˆ
- âœ… **æç¤ºè¯èåˆ**ï¼šæ”¹è¿›å¤šå› å­èåˆç®—æ³•
- âœ… **ç•Œé¢ä¼˜åŒ–**ï¼šè°ƒæ•´æ–‡æœ¬æ¡†é«˜åº¦ï¼Œæå‡ç”¨æˆ·ä½“éªŒ
- âœ… **ä¾èµ–æ›´æ–°**ï¼šç²¾ç®€requirements.txtï¼Œç§»é™¤æœªä½¿ç”¨ä¾èµ–

## è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨MITè®¸å¯è¯ï¼Œè¯¦è§LICENSEæ–‡ä»¶ã€‚

## è´¡çŒ®

æ¬¢è¿æäº¤Issueå’ŒPull Requestæ¥æ”¹è¿›é¡¹ç›®ã€‚

---

**CRB - è®©AIé™ªä¼´æ›´æœ‰æ¸©åº¦** ğŸŒŸ
=======
è¿è¡Œä»¥ä¸Šå‘½ä»¤å³å¯éªŒè¯å„æ¨¡å—å’Œæ•´ä½“ç³»ç»Ÿçš„åŸºæœ¬åŠŸèƒ½ã€‚

>>>>>>> c0a2bd3e05c9111a727cf054f8dccbae9279c63b
